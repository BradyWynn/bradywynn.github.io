<!DOCTYPE html>
<link rel="stylesheet" href="style.css" />

<body>
  <h2>background</h2>
  <p>
    Over the summer neuralink held a
    <a href="https://content.neuralink.com/compression-challenge/README.html"
      >compression challenge</a
    >
    where 200x lossless compression was needed to send electrode recordings
    wirelessly. This was
    <a href="https://x.com/felix_red_panda/status/1794027648257020292"
      >mocked</a
    >
    as being impossible however it sparked a similar challenge from
    <a href="https://comma.ai/">comma.ai</a>, a company developing an open
    source autopilot. This blogpost explains my submission to the challenge that
    is comparable to second place.
  </p>
  <h2>the challenge</h2>
  <img
    src="attachment\Screenshot 2025-02-08 225622.png"
    ,
    class="img_responsive"
    width="700"
  />
  <p>
    The challenge is to "losslessly compress 5,000 minutes of driving data", an
    example clip of raw driving video is shown below.
  </p>
  <br />
  <video width="700" controls class="img_responsive">
    <source
      src="attachment\249573591-91894bf7-592b-4204-b3f2-3e805984045c.mp4"
      ,
      type="video/mp4"
    />
  </video>
  <br />
  <p>
    This isn't actually the data provided. Even though the video is quite low
    resolution, only 256x128 the full 5000 minutes would be very large so to
    make the data easier to handle it has already been heavily compressed using
    a
    <a href="https://arxiv.org/pdf/1711.00937">VQ-VAE</a>.
  </p>
  <img src="attachment\image2274.png" class="img_responsive" width="700" />
  <p>
    Using the VQ-VAE network the video is compressed from raw 256x128 frames to
    10 bit 16x8 frames. This intermediate frame is called the latent
    representation and it being 10 bit just means its values can take on any
    integer from 0 to 1023. If we only store the latent representation and
    decoder we can recover the original image, however note that this is lossy
    compression which means information is lost when compressing. Below is the
    same clip as before but now compressed with the VQ-VAE.
  </p>
  <br />
  <video width="700" controls class="img_responsive">
    <source
      src="attachment\249573605-3a799ac8-781e-461c-bf14-c15cea42b985.mp4"
      ,
      type="video/mp4"
    />
  </video>
  <br />
  <p>
    Since each frame is compressed independently there is temporal artifacting
    between frames but its captures the general features of the image. Something
    cool is that we can find features in the latent representation directly.
    Below is the average of all the latent frames and there is a visible horizon
    and road lines even though it is extremely low resoluton. It is also
    interesting that all of the values are in range of 500 to 525.
  </p>
  <img src="attachment\image.png" class="img_responsive" />
  <p>
    Before compression the data is about ~915 mb and the first place submission
    has a compression ratio of 3.4 so they were able to get it down to about 270
    mb.
  </p>
  <h2>arithmetic coding</h2>
  <p>
    Arithmetic coding is a popular lossless compression technique. Lossless here
    just means that no information is lost and is different from lossy
    compression like jpeg that loses quality when compressed. To begin,
    arithmetic coding first needs a model that assigns a probability to each
    character of how likely it is to appear. An example model is shown below.
  </p>
  <br />
  <table>
    <tr>
      <th>Character</th>
      <th>Probability</th>
    </tr>
    <tr>
      <th>a</th>
      <th>0.2</th>
    </tr>
    <tr>
      <th>b</th>
      <th>0.3</th>
    </tr>
    <tr>
      <th>c</th>
      <th>0.5</th>
    </tr>
  </table>
  <br />
  <p>
    We also need a message to compress and in this case that will simply be
    "bac". To begin encoding the message we split the range from 0 to 1 into
    three different parts corresponding to our characters, each intervals width
    is dependent on its probability. Since the model assigns the highest
    probability to c it has the largest interval and a and b has smaller
    intervals. The first letter of the message is "b" so the message has to be
    contained within the interval from 0.2 to 0.5. Since we know that the
    message has to be in that interval we can again subdivide it according to
    our characters and their probabilities. The second letter is "a" so the
    message must be contained within the "a" interval and so we then further it
    into the three parts. We continue like this until the message has been fully
    encoded. The final interval that our message falls in is from 0.23 to 0.26
    because for every single point inside the interval the first three letters
    of the message are "bac".
  </p>
  <img src="attachment\encode 2.png" class="img_responsive" , width="700" />
  <p>
    However we haven't actually compressed the message yet. Next we need to
    convert this range into a binary sequence that we can store. To do this we
    are going to split the interval from 0 to 1 in half, if the interval of our
    message falls below 0.5 we record a 0 and above a 1. For our example since
    the interval 0.23 to 0.26 is below 0.5 we record a 0 and split the range
    from 0 to 0.5 in half. We continually do this until the binary sequence is
    entirely contained within the interval of 0.23 to 0.26. For the message
    "bac" the encoding is 001111.
  </p>

  <img src="attachment\binary.png" class="img_responsive" , width="700" />
  <p>
    To decode the encoded message you just do the reverse of the encoding. First
    convert the binary 001111 to decimal to get 0.234375. Since this falls
    within the range of 0.2 to 0.5 the first letter is "b". Similarly to the
    encoding this is continually done until the message is recovered. However
    you also need an end condition because you could continue decoding a message
    forever. You can either define a specific character in the vocabulary to be
    an end of sequence character or simply stop at a specific length.
  </p>
  <p>
    What was described above is called infinite precision arithmetic coding
    because it requires infinite precision. Computers obviously do not have
    infinite precision and if you were try and implement this in code it would
    fail for longer sequences because the intervals would become so tiny that
    the computer would not be able to represent them properly.
    <a
      href="https://web.archive.org/web/20241123060529/https://marknelson.us/posts/2014/10/19/data-compression-with-arithmetic-coding.html"
      >This</a
    >
    excellent blogpost explains the finite precision version of the algorithm
    that works for longer sequences and that I used for my implementation.
  </p>
  <h2>the model</h2>
  <p>
    As previously mentioned for arithmetic coding to work you need a model to
    provide probabilities. Conveniently enough, included in the repository with
    the driving data is a gpt-2 medium sized model (~350 million parameters)
    that is trained on 3 million minutes of driving data to predict the next
    token. Specifically what they did was flatten the 10 bit 16x8 latent
    representation into 128 "tokens" and the model takes in a fixed content
    length of the n past tokens and produces a probability distribution across
    all of the 1024 possible values of the next token. So we can actually just
    plug this model directly into arithmetic coding and use its probabilities to
    compress the data, and this actually isn't a novel idea.
    <a href="https://arxiv.org/pdf/2309.10668">This</a> paper proposed exactly
    this method but with language models and text compression and was actually
    where I got this idea from. So this works and is great but there is actually
    two major problems. The first is that the model itself takes up 614 mb so
    even if the model compressed the data into 0 bits we would still have to
    store the model and it would be twice the size as the first place
    submission. The second and related problem is that because the model is so
    large it is extremely slow. On a good gpu it runs at 0.3 seconds per frame
    which doesn't actually sound that bad. But then you remember that there are
    5000 minutes and 1200 frames per minute it comes out to 21 days which is
    obviously not ideal.
  </p>
  <h2>architecture improvements</h2>
  <p>
    So at this point its pretty clear we are going to have to train our own
    model. This model will almost certainly be worse than the gpt-2 medium sized
    model but if there is any hope of achieveing a good compression ratio and
    fast speeds it needs to be done.
  </p>
  <p>
    An interesting observation is that the model is predicting one token at a
    time. We can actually make an independence assumption that tokens within a
    single frame won't really help predicting other tokens within that same
    frame. What is really important is what that token was in previous frames.
    In this way we can change the transformer to produce probabilities for every
    single token in a frame at a single time. Predicting on a frame level
    instead of token level. This is actually a really great idea because it
    should actually make our model 128 times faster for essentially free.
  </p>
</body>
