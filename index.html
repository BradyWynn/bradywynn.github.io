<!DOCTYPE html>
<link rel="stylesheet" href="style.css" />

<body>
  <h2>background</h2>
  <p>
    Over the summer neuralink held a
    <a href="https://content.neuralink.com/compression-challenge/README.html"
      >compression challenge</a
    >
    where 200x lossless compression was needed to send electrode recordings
    wirelessly. This was
    <a href="https://x.com/felix_red_panda/status/1794027648257020292"
      >mocked</a
    >
    as being impossible however it sparked a similar challenge from
    <a href="https://comma.ai/">comma.ai</a>, a company developing an open
    source autopilot. This blogpost explains my submission to the challenge that
    is comparable to second place.
  </p>
  <h2>the challenge</h2>
  <img
    src="attachment\Screenshot 2025-02-08 225622.png"
    ,
    class="img_responsive"
    width="700"
  />
  <p>
    The challenge is to "losslessly compress 5,000 minutes of driving data", an
    example clip of raw driving video is shown below.
  </p>
  <br />
  <video width="700" controls class="img_responsive">
    <source
      src="attachment\249573591-91894bf7-592b-4204-b3f2-3e805984045c.mp4"
      ,
      type="video/mp4"
    />
  </video>
  <br />
  <p>
    This isn't actually the data provided. Even though the video is quite low
    resolution, only 256x128 the full 5,000 minutes would be very large so to
    make the data easier to handle it has already been heavily compressed using
    a
    <a href="https://arxiv.org/pdf/1711.00937">VQ-VAE</a>.
  </p>
  <img src="attachment\image2274.png" class="img_responsive" width="700" />
  <p>
    Using the VQ-VAE network the video is compressed from raw 256x128 frames to
    10 bit 16x8 frames. This intermediate frame is called the latent
    representation and it being 10 bit just means its values can take on any
    integer from 0 to 1023. If we only store the latent representation and
    decoder we can recover the original image, however note that this is lossy
    compression which means information is lost when compressing. Below is the
    same clip as before but now compressed with the VQ-VAE.
  </p>
  <br />
  <video width="700" controls class="img_responsive">
    <source
      src="attachment\249573605-3a799ac8-781e-461c-bf14-c15cea42b985.mp4"
      ,
      type="video/mp4"
    />
  </video>
  <br />
  <p>
    Since each frame is compressed independently there is temporal artifacting
    between frames but its captures the general features of the image. Something
    cool is that we can find features in the latent representation directly.
    Below is the average of all the latent frames and there is a visible horizon
    and road lines even though it is extremely low resoluton. It is also
    interesting that all of the values are in range of 500 to 525.
  </p>
  <img src="attachment\image.png" class="img_responsive" />
  <p>
    Before compression the data is about ~915 mb and the first place submission
    has a compression ratio of 3.4 so they were able to get it down to about 270
    mb.
  </p>
  <h2>arithmetic coding</h2>
  <p>
    Arithmetic coding is a popular lossless compression technique. Lossless here
    just means that no information is lost and is different from lossy
    compression like jpeg that loses quality when compressed. To begin,
    arithmetic coding first needs a model that assigns a probability to each
    character of how likely it is to appear. An example model is shown below.
  </p>
  <br />
  <table>
    <tr>
      <th>Character</th>
      <th>Probability</th>
    </tr>
    <tr>
      <th>a</th>
      <th>0.2</th>
    </tr>
    <tr>
      <th>b</th>
      <th>0.3</th>
    </tr>
    <tr>
      <th>c</th>
      <th>0.5</th>
    </tr>
  </table>
  <br />
  <p>
    We also need a message to compress and in this case that will simply be
    "bac". To begin encoding the message we split the range from 0 to 1 into
    three different parts corresponding to our characters, each intervals width
    is dependent on its probability. Since the model assigns the highest
    probability to c it has the largest interval and a and b has smaller
    intervals. The first letter of the message is "b" so the message has to be
    contained within the interval from 0.2 to 0.5. Since we know that the
    message has to be in that interval we can again subdivide it according to
    our characters and their probabilities. The second letter is "a" so the
    message must be contained within the "a" interval and so we then further it
    into the three parts. We continue like this until the message has been fully
    encoded. The final interval that our message falls in is from 0.23 to 0.26
    because for every single point inside the interval the first three letters
    of the message are "bac".
  </p>
  <img src="attachment\encode 2.png" class="img_responsive" , width="700" />
  <p>
    However we haven't actually compressed the message yet. Next we need to
    convert this range into a binary sequence that we can store. To do this we
    are going to split the interval from 0 to 1 in half, if the interval of our
    message falls below 0.5 we record a 0 and above a 1. For our example since
    the interval 0.23 to 0.26 is below 0.5 we record a 0 and split the range
    from 0 to 0.5 in half. We continually do this until the binary sequence is
    entirely contained within the interval of 0.23 to 0.26. For the message
    "bac" the encoding is 001111.
  </p>

  <img src="attachment\binary.png" class="img_responsive" , width="700" />
  <p>
    To decode the encoded message you just do the reverse of the encoding. First
    convert the binary 001111 to decimal to get 0.234375. Since this falls
    within the range of 0.2 to 0.5 the first letter is "b". Similarly to the
    encoding this is continually done until the message is recovered. However
    you also need an end condition because you could continue decoding a message
    forever. You can either define a specific character in the vocabulary to be
    an end of sequence character or simply stop at a specific length.
  </p>
  <p>
    What was described above is called infinite precision arithmetic coding
    because it requires infinite precision. Computers obviously do not have
    infinite precision and if you were try and implement this it would fail for
    longer sequences because the intervals would become so tiny that the
    computer would not be able to represent them properly.
    <a
      href="https://web.archive.org/web/20241123060529/https://marknelson.us/posts/2014/10/19/data-compression-with-arithmetic-coding.html"
      >This</a
    >
    excellent blogpost explains the finite precision version of the algorithm
    that works for longer sequences and that I used for my implementation.
  </p>
  <h2>the model</h2>
  <p>
    As previously mentioned for arithmetic coding to work you need a model to
    provide probabilities for each symbol. Conveniently enough, included in the
    repository with the driving data is a gpt-2 medium sized model (~350 million
    parameters) that is trained on 3 million minutes of driving data to predict
    the next token. Specifically what they did was flatten the 10 bit 16x8
    latent representation into 128 "tokens" and the model takes in a fixed
    content length of the past n tokens and produces a probability distribution
    across all of the 1024 possible values of the next token. The full 5,000
    minutes of driving data is 768 million tokens (5,000 minutes * 1,200 frames
    per minute * 128 tokens per frame).
  </p>
  <p>
    So we can actually just plug this model directly into arithmetic coding and
    use its probabilities to compress the data, and it works, but there are two
    major problems. The first is that the model itself takes up 614 mb so even
    just storing the model would take twice the space of the first place
    submission. The second and related problem is that because the model is so
    large it is extremely slow. On a my gpu (rtx 3070 ti) it runs at 130 tokens per
    second which doesn't sound that bad, but then you remember that there are
    768 million tokens so it would take 68 days.
  </p>
  <p>
    As an aside this isn't actually a novel idea, and
    <a href="https://arxiv.org/pdf/2309.10668">this</a> paper proposed exactly
    this method but with language models and text compression instead of driving
    data.
  </p>
  <h2>baseline model</h2>
  <p>
    So at this point its pretty clear we are going to have to train our own
    model. This model will almost certainly be worse than the gpt-2 medium sized
    model but if there is any hope of achieveing a good compression ratio and
    fast speeds it needs to be done.
  </p>
  <p>
    Alongside the 5,000 minutes of driving data we need to compress there is
    another 95,000 minutes of driving data we can use for training. The
    first baseline model has around 3.7 million parameters and is trained on 1
    epoch of the driving data. Most of the training details are rather
    unimportant but I will say shuffling the data and using Muon instead of
    AdamW were the two largest performance jumps. Below is the loss
    graph and when it finishes training the loss is about 2.5.
  </p>

  <img
    src="attachment/W&B Chart 2_15_2025, 1_49_38 PM.png"
    class="img_responsive"
    ,
    width="700"
  />

  <p>
    The cool thing about cross entropy is that we can actually use the loss
    to estimate how well it will compress. The cross entropy implementation
    in pytorch uses natural log but if we convert it to base 2 we get that the model
    needs about 3.6 bits per symbol. Since each symbol normally takes 10 bits 
    we can do 10/3.6 to get a compression ratio of about 2.78. We also need to 
    store the model which is about 7 mb so the actual compression ratio will be 
    around 2.7.
  </p>

  <p>
    So it seems the size of the model is no longer an issue however it is still 
    painfully slow. It can now generate at 908 tokens per second which is certainly
    an improvement but would still take 10 days.
  </p>

  <h2>architecture improvements</h2>
  <p>
    If speed is an issue we can always just train a smaller model, however 
    the performance drops as it gets smaller. Instead of shrinking the model
    we can actually change what exactly its predicting.
  </p>
  <p>
    An interesting observation is that comma's model is only predicting one
    token at a time. We can actually make an independence assumption that tokens
    within a frame won't help predict other tokens within the same frame. The
    only thing that matters is what that token was in previous frames. In this
    way we can change the transformer to produce probabilities for every single
    token in a frame at a single time. Predicting on a frame level instead of
    token level. This is actually a really great idea because it should actually
    make our model 128 times faster for essentially free.
  </p>

  <p>
    Just to digress for a second about how comma's model actually works. Each
    frame is 128 tokens however if you were to just feed the model a stream of
    tokens it would not perform well because it would not know when a frame
    begins or ends. So there is a special token designated "beginning of
    sequence" or BOS that seperates different frames from each other. However
    when predicting entire frames we can actually completely do away with the
    BOS and just add another positional embedding but for frame index instead of
    token index. Below is the cross entropy loss plot for two different models,
    one trained with the BOS token on a token level and one without the BOS
    token trained to predict on a frame level. The loss of both models
    eventually converges and so it seems to validate the independence
    assumption.
  </p>

  <img
    src="attachment\W&B Chart 2_15_2025, 12_15_20 AM (1).png"
    class="img_responsive"
    ,
    width="700"
  />

  <p>
    For reference these models are the same architecture as the gpt-2 model but
    now about 3.7 million parameters.
  </p>
</body>
